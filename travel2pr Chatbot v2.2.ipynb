{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import streamlit as st\n",
    "from time import sleep\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.tools import Tool\n",
    "from io import BytesIO\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.utils import simpleSplit\n",
    "\n",
    "# ğŸ“Œ Load API keys from environment variables\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# ğŸ“Œ Initialize embedding model and LLM\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY)\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0, model=\"gpt-4-turbo\")\n",
    "\n",
    "# ğŸ“Œ Load the vector database\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"landmarks_rag\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# ğŸ“Œ Define a custom prompt for the chatbot\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],  \n",
    "    template=\"\"\"\n",
    "You are a Puerto Rico travel assistant. \n",
    "Use the retrieved information to provide the best travel recommendations in a natural and engaging way.\n",
    "First Welcome the user and ask for their preferences and details that will help you to provide the best recommendations. \\\n",
    "ENSURE to ask as much information as possible to provide the best recommendations. ALWAYS ask follow-up questions to lock user's selections. \\\n",
    "GUIDE the user through the process of selecting the best landmarks based on their preferences. \\\n",
    "\n",
    "Questions you can ask:\n",
    "- How many days do you plan to stay?\n",
    "- What type of activities do you enjoy?\n",
    "- Are you traveling with family or alone?\n",
    "- Do you have any specific dietary restrictions?\n",
    "- Do you prefer outdoor or indoor activities?\n",
    "\n",
    "Follow-up questions:\n",
    "- Which of these landmarks are you most interested in?\n",
    "- Are you interested in historical sites or natural landmarks?\n",
    "- Do you prefer adventurous activities or relaxing experiences?\n",
    "- Do you want to include any of these landmarks in your itinerary?\n",
    "- Would you like to visit any of these landmarks?\n",
    "\n",
    "### Context (Relevant Information from RAG):\n",
    "{context}\n",
    "\n",
    "### Instructions:\n",
    "- Provide a well-structured travel recommendation based on the retrieved landmarks.\n",
    "- Ensure continuity with previous discussions.\n",
    "- Prioritize landmarks that match the userâ€™s preferences.\n",
    "- If multiple options exist, suggest the BEST ones with reasoning.\n",
    "- Avoid repeating information already given in the conversation.\n",
    "- In the end ask the user which of these locations you will like to visit.\n",
    "\n",
    "### User Question:\n",
    "{question}\n",
    "\n",
    "### AI Response:\n",
    "Based on the information available, hereâ€™s what I recommend:\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ğŸ“Œ Configure the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt}\n",
    ")\n",
    "\n",
    "# ğŸ“Œ Apply UI settings\n",
    "st.set_page_config(page_title=\"AI Travel2PR Planner\", page_icon=\"ğŸŒ\", layout=\"centered\")\n",
    "\n",
    "# ğŸ“Œ Display a header\n",
    "st.markdown(\"<h1>ğŸ—ºï¸ AI Travel to Puerto Rico Planner ğŸï¸</h1>\", unsafe_allow_html=True)\n",
    "st.markdown(\"<p>Ask for recommendations about places to visit in Puerto Rico!</p>\", unsafe_allow_html=True)\n",
    "\n",
    "# ğŸ“Œ Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "if \"itinerary_data\" not in st.session_state:\n",
    "    st.session_state[\"itinerary_data\"] = []\n",
    "\n",
    "# ğŸ“Œ Function to reset conversation\n",
    "def reset_conversation():\n",
    "    st.session_state[\"messages\"] = []\n",
    "    st.session_state[\"user_input\"] = \"\"\n",
    "    st.session_state.pop(\"final_itinerary\", None)\n",
    "    st.session_state[\"itinerary_data\"] = []  # Clear itinerary data\n",
    "    st.rerun()\n",
    "\n",
    "st.button(\"ğŸ”„ Start a New Consultation\", on_click=reset_conversation)\n",
    "\n",
    "# ğŸ“Œ Function to process user input\n",
    "def process_input():\n",
    "    user_query = st.session_state[\"user_input\"].strip()\n",
    "    \n",
    "    if user_query:\n",
    "        with st.spinner(\"ğŸ” Searching for travel recommendations...\"):\n",
    "            response = qa_chain.run(user_query)\n",
    "        \n",
    "        # Append user message\n",
    "        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": user_query})\n",
    "        st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        # Store relevant places\n",
    "        st.session_state[\"itinerary_data\"].append(response) \n",
    "        \n",
    "        # Clear input field\n",
    "        st.session_state.update({\"user_input\": \"\"})\n",
    "        st.rerun()\n",
    "\n",
    "# ğŸ“Œ Display conversation history\n",
    "for message in st.session_state[\"messages\"]:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# ğŸ“Œ User input field below chat messages\n",
    "st.text_input(\"âœï¸ Type your question and press Enter:\", \"\", key=\"user_input\", on_change=process_input)\n",
    "\n",
    "# ğŸ“Œ Generate formatted PDF itinerary\n",
    "def generate_pdf(itinerary_text):\n",
    "    if not itinerary_text.strip():\n",
    "        return None  \n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    c = canvas.Canvas(buffer, pagesize=letter)\n",
    "    c.setFont(\"Helvetica-Bold\", 14)\n",
    "    c.drawString(200, 750, \"Puerto Rico Travel Itinerary\")\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    \n",
    "    y_position = 700\n",
    "    for line in itinerary_text.split(\"\\n\"):\n",
    "        wrapped_lines = simpleSplit(line, \"Helvetica\", 12, 500)\n",
    "        for wrapped_line in wrapped_lines:\n",
    "            c.drawString(50, y_position, wrapped_line)\n",
    "            y_position -= 20  \n",
    "            if y_position < 50:  \n",
    "                c.showPage()\n",
    "                c.setFont(\"Helvetica\", 12)\n",
    "                y_position = 750\n",
    "    \n",
    "    c.save()\n",
    "    buffer.seek(0)\n",
    "    return buffer\n",
    "\n",
    "# ğŸ“Œ Button to download the itinerary as a PDF\n",
    "if st.button(\"ğŸ“„ Download Itinerary as PDF\"):\n",
    "    if \"itinerary_data\" in st.session_state and st.session_state[\"itinerary_data\"]:\n",
    "        pdf_text = \"\\n\".join(st.session_state[\"itinerary_data\"])\n",
    "        pdf_buffer = generate_pdf(pdf_text)\n",
    "        st.download_button(\n",
    "            label=\"ğŸ“¥ Click to Download\",\n",
    "            data=pdf_buffer,\n",
    "            file_name=\"Puerto_Rico_Itinerary.pdf\",\n",
    "            mime=\"application/pdf\"\n",
    "        )\n",
    "    else:\n",
    "        st.warning(\"âš ï¸ No itinerary available to download. Ask the AI for recommendations first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 20:13:03.134 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.139 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.346 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-13 20:13:03.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.348 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.351 Session state does not function when running a script without `streamlit run`\n",
      "2025-02-13 20:13:03.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.357 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.358 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.358 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.359 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.360 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.361 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.363 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.364 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.365 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.365 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.366 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.367 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.369 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.370 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.373 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.375 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.375 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.376 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.377 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.379 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.381 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.383 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-13 20:13:03.388 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import streamlit as st\n",
    "from time import sleep\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain, RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.tools import Tool\n",
    "from io import BytesIO\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.utils import simpleSplit\n",
    "\n",
    "# ğŸ“Œ Load API keys from environment variables\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "WEATHER_API_KEY = os.getenv(\"WEATHER_API_KEY\")\n",
    "\n",
    "# ğŸ“Œ Initialize embedding model and LLM\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY)\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0, model=\"gpt-4-turbo\")\n",
    "\n",
    "# ğŸ“Œ Load the vector database\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"landmarks_rag\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# ğŸ“Œ Memory Tool for Tracking User Preferences\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# ğŸ“Œ Define a custom prompt for the chatbot\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"],  \n",
    "    template=\"\"\"\n",
    "You are a Puerto Rico travel assistant. \n",
    "Use the retrieved information to provide the best travel recommendations in a natural and engaging way.\n",
    "First Welcome the user and ask for their preferences and details that will help you to provide the best recommendations. \\\n",
    "ENSURE to ask as much information as possible to provide the best recommendations. ALWAYS ask follow-up questions to lock user's selections. \\\n",
    "GUIDE the user through the process of selecting the best landmarks based on their preferences. \\\n",
    "\n",
    "### User Preferences Stored:\n",
    "{chat_history}\n",
    "\n",
    "### Context (Relevant Information from RAG):\n",
    "{context}\n",
    "\n",
    "### Instructions:\n",
    "- Provide a well-structured travel recommendation based on the retrieved landmarks.\n",
    "- Ensure continuity with previous discussions.\n",
    "- Prioritize landmarks that match the userâ€™s preferences.\n",
    "- If multiple options exist, suggest the BEST ones with reasoning.\n",
    "- Avoid repeating information already given in the conversation.\n",
    "- In the end ask the user which of these locations you will like to visit.\n",
    "\n",
    "### User Question:\n",
    "{question}\n",
    "\n",
    "### AI Response:\n",
    "Based on the information available, hereâ€™s what I recommend:\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ğŸ“Œ Configure the RetrievalQA chain with Memory\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt, \"memory\": memory}\n",
    ")\n",
    "\n",
    "# ğŸ“Œ Apply UI settings\n",
    "st.set_page_config(page_title=\"AI Travel2PR Planner\", page_icon=\"ğŸŒ\", layout=\"centered\")\n",
    "\n",
    "# ğŸ“Œ Display a header\n",
    "st.markdown(\"<h1>ğŸ—ºï¸ AI Travel to Puerto Rico Planner ğŸï¸</h1>\", unsafe_allow_html=True)\n",
    "st.markdown(\"<p>Ask for recommendations about places to visit in Puerto Rico!</p>\", unsafe_allow_html=True)\n",
    "\n",
    "# ğŸ“Œ Initialize session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "if \"itinerary_data\" not in st.session_state:\n",
    "    st.session_state[\"itinerary_data\"] = []\n",
    "if \"user_preferences\" not in st.session_state:\n",
    "    st.session_state[\"user_preferences\"] = []\n",
    "\n",
    "# ğŸ“Œ Function to reset conversation\n",
    "def reset_conversation():\n",
    "    st.session_state[\"messages\"] = []\n",
    "    st.session_state[\"user_input\"] = \"\"\n",
    "    st.session_state.pop(\"final_itinerary\", None)\n",
    "    st.session_state[\"itinerary_data\"] = []  # Clear itinerary data\n",
    "    st.session_state[\"user_preferences\"] = []  # Clear stored user preferences\n",
    "    memory.clear()  # Clear memory tool\n",
    "    st.rerun()\n",
    "\n",
    "st.button(\"ğŸ”„ Start a New Consultation\", on_click=reset_conversation)\n",
    "\n",
    "# ğŸ“Œ Function to fetch weather for selected locations\n",
    "def get_weather(location):\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={WEATHER_API_KEY}&units=metric\"\n",
    "    response = requests.get(url).json()\n",
    "    if response.get(\"weather\"):\n",
    "        return f\"{response['weather'][0]['description']}, Temp: {response['main']['temp']}Â°C\"\n",
    "    return \"Weather data unavailable\"\n",
    "\n",
    "# ğŸ“Œ Function to process user input\n",
    "def process_input():\n",
    "    user_query = st.session_state[\"user_input\"].strip()\n",
    "    \n",
    "    if user_query:\n",
    "        with st.spinner(\"ğŸ” Searching for travel recommendations...\"):\n",
    "            response = qa_chain.invoke({\"question\": user_query})\n",
    "        \n",
    "        # Append user message\n",
    "        st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": user_query})\n",
    "        st.session_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response[\"result\"]})  # Extract only the text response\n",
    "        \n",
    "        # Store relevant places\n",
    "        st.session_state[\"itinerary_data\"].append(response[\"result\"]) \n",
    "        \n",
    "        # Capture user preferences for future queries\n",
    "        st.session_state[\"user_preferences\"].append(user_query)\n",
    "        memory.save_context({\"input\": user_query}, {\"output\": response[\"result\"]})\n",
    "        \n",
    "        # Clear input field\n",
    "        st.session_state.update({\"user_input\": \"\"})\n",
    "        st.rerun()\n",
    "\n",
    "# ğŸ“Œ Display conversation history\n",
    "for message in st.session_state[\"messages\"]:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# ğŸ“Œ User input field below chat messages\n",
    "st.text_input(\"âœï¸ Type your question and press Enter:\", \"\", key=\"user_input\", on_change=process_input)\n",
    "\n",
    "# ğŸ“Œ Generate formatted PDF itinerary\n",
    "def generate_pdf(itinerary_text):\n",
    "    if not itinerary_text.strip():\n",
    "        return None  \n",
    "    \n",
    "    buffer = BytesIO()\n",
    "    c = canvas.Canvas(buffer, pagesize=letter)\n",
    "    c.setFont(\"Helvetica-Bold\", 14)\n",
    "    c.drawString(200, 750, \"Puerto Rico Travel Itinerary\")\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    \n",
    "    y_position = 700\n",
    "    for line in itinerary_text.split(\"\\n\"):\n",
    "        wrapped_lines = simpleSplit(line, \"Helvetica\", 12, 500)\n",
    "        for wrapped_line in wrapped_lines:\n",
    "            c.drawString(50, y_position, wrapped_line)\n",
    "            y_position -= 20  \n",
    "            if y_position < 50:  \n",
    "                c.showPage()\n",
    "                c.setFont(\"Helvetica\", 12)\n",
    "                y_position = 750\n",
    "    \n",
    "    c.save()\n",
    "    buffer.seek(0)\n",
    "    return buffer\n",
    "\n",
    "# ğŸ“Œ Button to download the itinerary as a PDF\n",
    "if st.button(\"ğŸ“„ Download Itinerary as PDF\"):\n",
    "    if \"itinerary_data\" in st.session_state and st.session_state[\"itinerary_data\"]:\n",
    "        pdf_text = \"\\n\".join(st.session_state[\"itinerary_data\"])\n",
    "        pdf_buffer = generate_pdf(pdf_text)\n",
    "        st.download_button(\n",
    "            label=\"ğŸ“¥ Click to Download\",\n",
    "            data=pdf_buffer,\n",
    "            file_name=\"Puerto_Rico_Itinerary.pdf\",\n",
    "            mime=\"application/pdf\"\n",
    "        )\n",
    "    else:\n",
    "        st.warning(\"âš ï¸ No itinerary available to download. Ask the AI for recommendations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Standalone Testing Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ºï¸ AI Travel Planner - Test Mode ğŸï¸\n",
      "Type 'exit' to stop.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One input key expected got ['question', 'input_documents']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(itinerary_data))\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 63\u001b[0m, in \u001b[0;36mprocess_input\u001b[1;34m(user_query)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_input\u001b[39m(user_query):\n\u001b[1;32m---> 63\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Store user preferences for later reference\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     user_preferences\u001b[38;5;241m.\u001b[39mappend(user_query)\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\base.py:606\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    607\u001b[0m         _output_key\n\u001b[0;32m    608\u001b[0m     ]\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    612\u001b[0m         _output_key\n\u001b[0;32m    613\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:154\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs}\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\base.py:611\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    607\u001b[0m         _output_key\n\u001b[0;32m    608\u001b[0m     ]\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    612\u001b[0m         _output_key\n\u001b[0;32m    613\u001b[0m     ]\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    618\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    619\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[1;32m--> 165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\chains\\base.py:466\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[1;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\memory\\chat_memory.py:72\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     input_str, output_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_messages(\n\u001b[0;32m     74\u001b[0m         [\n\u001b[0;32m     75\u001b[0m             HumanMessage(content\u001b[38;5;241m=\u001b[39minput_str),\n\u001b[0;32m     76\u001b[0m             AIMessage(content\u001b[38;5;241m=\u001b[39moutput_str),\n\u001b[0;32m     77\u001b[0m         ]\n\u001b[0;32m     78\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\memory\\chat_memory.py:47\u001b[0m, in \u001b[0;36mBaseChatMemory._get_input_output\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_input_output\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]\n\u001b[0;32m     45\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 47\u001b[0m         prompt_input_key \u001b[38;5;241m=\u001b[39m \u001b[43mget_prompt_input_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         prompt_input_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key\n",
      "File \u001b[1;32mc:\\Users\\larry\\anaconda3\\envs\\travel2pr_ai\\lib\\site-packages\\langchain\\memory\\utils.py:19\u001b[0m, in \u001b[0;36mget_prompt_input_key\u001b[1;34m(inputs, memory_variables)\u001b[0m\n\u001b[0;32m     17\u001b[0m prompt_input_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(inputs)\u001b[38;5;241m.\u001b[39mdifference(memory_variables \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_input_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne input key expected got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_input_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_input_keys[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: One input key expected got ['question', 'input_documents']"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# ğŸ“Œ Load API keys\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "WEATHER_API_KEY = os.getenv(\"WEATHER_API_KEY\")\n",
    "\n",
    "# ğŸ“Œ Initialize models and vectorstore\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY)\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0, model=\"gpt-4-turbo\")\n",
    "vectorstore = Chroma(collection_name=\"landmarks_rag\", embedding_function=embedding_model)\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# ğŸ“Œ Memory tool to retain user preferences\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# ğŸ“Œ Define a custom prompt template\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"],  \n",
    "    template=\"\"\"\n",
    "You are a Puerto Rico travel assistant. \n",
    "Use the retrieved information to provide the best travel recommendations in a natural and engaging way.\n",
    "Ensure you ask follow-up questions to refine the user's itinerary.\n",
    "\n",
    "### User Preferences Stored:\n",
    "{chat_history}\n",
    "\n",
    "### Context (Relevant Information from RAG):\n",
    "{context}\n",
    "\n",
    "### Instructions:\n",
    "- Provide structured travel recommendations.\n",
    "- Retain user preferences and integrate them into responses.\n",
    "- Prioritize highly relevant landmarks.\n",
    "- Suggest only the best options.\n",
    "\n",
    "### User Question:\n",
    "{question}\n",
    "\n",
    "### AI Response:\n",
    "Based on the information available, hereâ€™s what I recommend:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# ğŸ“Œ Configure the RetrievalQA chain with memory\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt, \"memory\": memory}\n",
    ")\n",
    "\n",
    "# ğŸ“Œ Function to fetch weather data\n",
    "def get_weather(location):\n",
    "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={WEATHER_API_KEY}&units=metric\"\n",
    "    response = requests.get(url).json()\n",
    "    if response.get(\"weather\"):\n",
    "        return f\"{response['weather'][0]['description']}, Temp: {response['main']['temp']}Â°C\"\n",
    "    return \"Weather data unavailable\"\n",
    "\n",
    "# ğŸ“Œ Simulated User Conversation (Test Cases)\n",
    "def test_chatbot():\n",
    "    print(\"\\nğŸ› ï¸ Running Chatbot V2.2 Test...\\n\")\n",
    "\n",
    "    # Step 1: User starts the conversation\n",
    "    user_input = \"Hello\"\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    response = qa_chain.run(user_input)\n",
    "    print(f\"\\nAI: {response}\\n\")\n",
    "\n",
    "    # Step 2: User provides trip details\n",
    "    user_input = \"I will be staying in Puerto Rico for 5 days.\"\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    response = qa_chain.run(user_input)\n",
    "    print(f\"\\nAI: {response}\\n\")\n",
    "\n",
    "    # Step 3: User selects activities\n",
    "    user_input = \"I like hiking and cultural activities. Traveling with my family.\"\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    response = qa_chain.run(user_input)\n",
    "    print(f\"\\nAI: {response}\\n\")\n",
    "\n",
    "    # Step 4: AI recommends landmarks\n",
    "    user_input = \"That sounds great. Show me more places to visit.\"\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    response = qa_chain.run(user_input)\n",
    "    print(f\"\\nAI: {response}\\n\")\n",
    "\n",
    "    # Step 5: User confirms landmark selection\n",
    "    user_input = \"I would like to visit El Yunque and Old San Juan.\"\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    response = qa_chain.run(user_input)\n",
    "    print(f\"\\nAI: {response}\\n\")\n",
    "\n",
    "    # Step 6: Fetch weather for selected landmarks\n",
    "    print(\"\\nğŸ“ Fetching weather data for El Yunque and Old San Juan...\")\n",
    "    weather_yunque = get_weather(\"El Yunque, PR\")\n",
    "    weather_sanjuan = get_weather(\"San Juan, PR\")\n",
    "    print(f\"\\nğŸŒ¦ï¸ El Yunque: {weather_yunque}\")\n",
    "    print(f\"ğŸŒ¦ï¸ Old San Juan: {weather_sanjuan}\\n\")\n",
    "\n",
    "    # Step 7: User requests final itinerary\n",
    "    user_input = \"Please generate my final itinerary.\"\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    response = qa_chain.run(user_input)\n",
    "    print(f\"\\nAI: {response}\\n\")\n",
    "\n",
    "    print(\"\\nâœ… Chatbot V2.2 Test Completed Successfully!\\n\")\n",
    "\n",
    "# ğŸ“Œ Run the test\n",
    "if __name__ == \"__main__\":\n",
    "    test_chatbot()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel2pr_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
