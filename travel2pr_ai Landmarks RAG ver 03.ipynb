{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from chromadb.config import Settings\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "OPENWEATHER_API_KEY = os.getenv('OPENWEATHER_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1: Release the database with the following code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = None  # Release the vector database\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: Restart the Kernel associated to the use of the Vectorstore database*\n",
    "\n",
    "*Step 3: Delete the vectorstore by running the following code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "VECTORSTORE_PATH = r\"C:\\Users\\larry\\chromadb_store\\landmarks_db\"  # Update to the actual path\n",
    "\n",
    "# Delete the existing database folder\n",
    "if os.path.exists(VECTORSTORE_PATH):\n",
    "    shutil.rmtree(VECTORSTORE_PATH)\n",
    "    print(\"Previous RAG deleted successfully.\")\n",
    "else:\n",
    "    print(\"No existing RAG found. Proceeding with a fresh build.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Data pre-treatment and Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON dataset\n",
    "json_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\pre-chunk clean data\\landmarks_step1a_updated_cat.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    landmarks_data = json.load(f)\n",
    "\n",
    "# Process and structure the data for ingestion\n",
    "processed_landmarks = []\n",
    "\n",
    "for entry in landmarks_data:\n",
    "    metadata = entry[\"metadata\"]\n",
    "    document_text = entry[\"page_content\"]\n",
    "    \n",
    "    # Extract relevant metadata fields\n",
    "    processed_entry = {\n",
    "        \"title\": metadata.get(\"title\", \"Unknown\"),\n",
    "        \"coordinates\": metadata.get(\"coordinates\", \"N/A\"),\n",
    "        \"categories\": metadata.get(\"categories\", \"\").split(\", \"),  # Convert categories to a list\n",
    "        \"relevant_links\": metadata.get(\"relevant_links\", \"\").split(\", \"),  # Convert links to a list\n",
    "        \"content\": document_text  # Store main content\n",
    "    }\n",
    "\n",
    "    processed_landmarks.append(processed_entry)\n",
    "\n",
    "# Convert processed data into a Pandas DataFrame\n",
    "df_landmarks = pd.DataFrame(processed_landmarks)\n",
    "\n",
    "# Display DataFrame information\n",
    "print(df_landmarks.head())  # Show the first few rows\n",
    "\n",
    "# Convert DataFrame to a NumPy array\n",
    "landmarks_array = df_landmarks.to_numpy()\n",
    "print(f\"Shape of NumPy array: {landmarks_array.shape}\")  # Display array shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_landmarks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics on the document lengths for evaluation and decision-making on the chunking strategy\n",
    "import numpy as np\n",
    "\n",
    "# Compute document lengths again\n",
    "lengths = [len(doc.split()) for doc in df_landmarks[\"content\"]]\n",
    "\n",
    "# Compute statistics\n",
    "avg_length = np.mean(lengths)\n",
    "max_length = np.max(lengths)\n",
    "min_length = np.min(lengths)\n",
    "std_dev = np.std(lengths)\n",
    "\n",
    "print(f\"Total landmarks processed: {len(df_landmarks)}\")\n",
    "print(f\"Average document length: {avg_length:.2f} words\")\n",
    "print(f\"Max length: {max_length} words | Min length: {min_length} words\")\n",
    "print(f\"Standard deviation of lengths: {std_dev:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_landmarks.to_csv(\"df_landmarks_backup.csv\", index=False)\n",
    "print(\"DataFrame saved as CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save(\"landmarks_array_backup.npy\", landmarks_array)\n",
    "print(\"NumPy array saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Chunking Data Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define chunking thresholds\n",
    "SMALL_DOC_THRESHOLD = 500\n",
    "MEDIUM_DOC_THRESHOLD = 2000\n",
    "\n",
    "# Function to determine chunking parameters\n",
    "def get_chunk_parameters(doc_length):\n",
    "    if doc_length <= SMALL_DOC_THRESHOLD:\n",
    "        return None  # No chunking needed\n",
    "    elif doc_length <= MEDIUM_DOC_THRESHOLD:\n",
    "        return {\"chunk_size\": 500, \"chunk_overlap\": 50}\n",
    "    else:\n",
    "        return {\"chunk_size\": 1000, \"chunk_overlap\": 100}\n",
    "\n",
    "# Apply chunking where needed\n",
    "chunked_documents = []\n",
    "\n",
    "for idx, row in df_landmarks.iterrows():\n",
    "    title = row[\"title\"]\n",
    "    coordinates = row[\"coordinates\"]\n",
    "    categories = \", \".join(row[\"categories\"])\n",
    "    relevant_links = \", \".join(row[\"relevant_links\"])\n",
    "    \n",
    "    document_text = row[\"content\"]\n",
    "    doc_length = len(document_text.split())\n",
    "\n",
    "    chunk_params = get_chunk_parameters(doc_length)\n",
    "    \n",
    "    if chunk_params is None:\n",
    "        # Small docs, store as is\n",
    "        chunked_documents.append({\n",
    "            \"content\": document_text,\n",
    "            \"metadata\": {\n",
    "                \"title\": title,\n",
    "                \"coordinates\": coordinates,\n",
    "                \"categories\": categories,\n",
    "                \"relevant_links\": relevant_links\n",
    "            }\n",
    "        })\n",
    "    else:\n",
    "        # Apply chunking\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_params[\"chunk_size\"],\n",
    "            chunk_overlap=chunk_params[\"chunk_overlap\"]\n",
    "        )\n",
    "        chunks = text_splitter.split_text(document_text)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunked_documents.append({\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"title\": title,\n",
    "                    \"coordinates\": coordinates,\n",
    "                    \"categories\": categories,\n",
    "                    \"relevant_links\": relevant_links\n",
    "                }\n",
    "            })\n",
    "\n",
    "print(f\"Total chunks created: {len(chunked_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 2a: Qucik Chunks Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first 5 chunks for verification\n",
    "for i, chunk in enumerate(chunked_documents[:5]):\n",
    "    print(f\"🔹 Chunk {i+1}: {len(chunk['content'].split())} words | Title: {chunk['metadata']['title']}\")\n",
    "    print(chunk['content'][:300])  # Preview first 300 chars\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 2b: Categories Distribution Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categories distribution\n",
    "from collections import Counter\n",
    "category_counts = Counter([doc['metadata']['categories'] for doc in chunked_documents])\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\chunked data\\landmarks_chunks_12feb25v03.json\"\n",
    "\n",
    "# Save chunked documents as JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_documents, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully saved {len(chunked_documents)} chunks to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(chunked_documents)} chunks\")\n",
    "print(\"Sample Chunk:\", chunked_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Initialize ChromaDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4: Create a Persistent ChromaDB Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "landmarksdb_path = r\"C:\\Users\\larry\\chromadb_store\\landmarks_db\"\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=landmarksdb_path)\n",
    "\n",
    "#chroma_client = chromadb.Client(Settings(\n",
    "#    chroma_db_impl=\"duckdb+parquet\", # Use DuckDB as database with Parquet storage format\n",
    "#    persist_directory=landmarksdb_path)) # Store the database in the specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 5: Create a Collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the collection\n",
    "collection_name = \"landmarks_rag\"\n",
    "\n",
    "# Check if the collection already exists and delete it to avoid duplication\n",
    "existing_collections = [col.name for col in chroma_client.list_collections()]\n",
    "if collection_name in existing_collections:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "# Create a new collection in ChromaDB\n",
    "collection = chroma_client.get_or_create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 6: Embeddings Preparation Using OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI's embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Prepare lists to store data before inserting into ChromaDB\n",
    "documents = []  # Store the main content (text) of each landmark\n",
    "metadatas = []  # Store metadata (title, coordinates, categories, etc.)\n",
    "ids = []        # Unique document IDs for ChromaDB\n",
    "embeddings = [] # Store generated embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 7: Process and Embed the Landmarks Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, doc in enumerate(chunked_documents):\n",
    "    doc_id = f\"chunk_{idx}\"  # Ensure unique ID per chunk\n",
    "    document_text = doc[\"content\"]\n",
    "\n",
    "    # Generate embedding for each chunk\n",
    "    embedding_vector = embedding_model.embed_query(document_text)\n",
    "\n",
    "    # Store metadata\n",
    "    metadata = doc[\"metadata\"]\n",
    "\n",
    "    # Store in lists for batch insertion\n",
    "    documents.append(document_text)\n",
    "    metadatas.append(metadata)\n",
    "    ids.append(doc_id)\n",
    "    embeddings.append(embedding_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 8: Insert Data into ChromaDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert chunked embeddings into ChromaDB\n",
    "collection.add(\n",
    "    ids=ids,                 # Unique document IDs\n",
    "    embeddings=embeddings,   # Precomputed embeddings\n",
    "    metadatas=metadatas,     # Metadata for filtering and retrieval\n",
    "    documents=documents      # Original landmark descriptions\n",
    ")\n",
    "\n",
    "# Confirm successful insertion by checking the number of documents stored\n",
    "print(f\"Number of chunks stored in ChromaDB: {collection.count()}\")\n",
    "print(f\"Number of documents chunks: {len(chunked_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 9: Perform a Test Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chroma' object has no attribute 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39membed_query(query_text)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Perform a similarity search in ChromaDB using the query embedding\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m retrieval_results \u001b[38;5;241m=\u001b[39m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m(\n\u001b[0;32m     17\u001b[0m     query_embeddings\u001b[38;5;241m=\u001b[39m[query_embedding],  \u001b[38;5;66;03m# Use embeddings instead of raw text\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     n_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Retrieve top 3 most relevant results\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Display retrieved results\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(retrieval_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Chroma' object has no attribute 'query'"
     ]
    }
   ],
   "source": [
    "# Ensure we use the same embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Define a sample query\n",
    "query_text = \"historical site with a large museum in Puerto Rico\"\n",
    "\n",
    "# Generate embedding for the query\n",
    "query_embedding = embedding_model.embed_query(query_text)\n",
    "\n",
    "# Perform a similarity search in ChromaDB using the query embedding\n",
    "retrieval_results = collection.query(\n",
    "    query_embeddings=[query_embedding],  # Use embeddings instead of raw text\n",
    "    n_results=3  # Retrieve top 3 most relevant results\n",
    ")\n",
    "\n",
    "# Display retrieved results\n",
    "for i, result in enumerate(retrieval_results[\"documents\"][0]):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"Title: {retrieval_results['metadatas'][0][i]['title']}\")\n",
    "    print(f\"Coordinates: {retrieval_results['metadatas'][0][i]['coordinates']}\")\n",
    "    print(f\"Categories: {retrieval_results['metadatas'][0][i]['categories']}\")\n",
    "    print(f\"Content: {result[:500]}...\")  # Show first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we use the same embedding model\n",
    "#embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Define a sample query\n",
    "query_text = \"best beaches for family vacations in Puerto Rico\"\n",
    "\n",
    "# Generate embedding for the query\n",
    "query_embedding = embedding_model.embed_query(query_text)\n",
    "\n",
    "# Perform a similarity search in ChromaDB using the query embedding\n",
    "retrieval_results = collection.query(\n",
    "    query_embeddings=[query_embedding],  # Use embeddings instead of raw text\n",
    "    n_results=5  # Retrieve top 3 most relevant results\n",
    ")\n",
    "\n",
    "# Display retrieved results\n",
    "for i, result in enumerate(retrieval_results[\"documents\"][0]):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"Title: {retrieval_results['metadatas'][0][i]['title']}\")\n",
    "    print(f\"Coordinates: {retrieval_results['metadatas'][0][i]['coordinates']}\")\n",
    "    print(f\"Categories: {retrieval_results['metadatas'][0][i]['categories']}\")\n",
    "    print(f\"Content: {result[:500]}...\")  # Show first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Update Your Retrieval Query to Include Similarity Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure query text is embedded\n",
    "query_embedding = embedding_model.embed_query(query_text)\n",
    "\n",
    "# Define a sample query\n",
    "query_text = \"Which is the top ranked beach in Puerto Rico?\"\n",
    "\n",
    "# Perform a similarity search and include the similarity scores\n",
    "retrieval_results = collection.query(\n",
    "    query_embeddings=[query_embedding],  \n",
    "    n_results=10,  # Retrieve top 5 results\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]  # Add similarity scores\n",
    ")\n",
    "\n",
    "# Display retrieved results with similarity scores\n",
    "for i, result in enumerate(retrieval_results[\"documents\"][0]):\n",
    "    similarity_score = 1 - retrieval_results[\"distances\"][0][i]  # Convert distance to similarity\n",
    "    print(f\"\\nResult {i+1}: (Similarity Score: {similarity_score:.4f})\")\n",
    "    print(f\"Title: {retrieval_results['metadatas'][0][i]['title']}\")\n",
    "    print(f\"Coordinates: {retrieval_results['metadatas'][0][i]['coordinates']}\")\n",
    "    print(f\"Categories: {retrieval_results['metadatas'][0][i]['categories']}\")\n",
    "    print(f\"Content: {result[:500]}...\")  # Show first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure query text is embedded\n",
    "query_embedding = embedding_model.embed_query(query_text)\n",
    "\n",
    "# Define a sample query\n",
    "query_text = \"Tell me about El Yunque.\"\n",
    "\n",
    "# Perform a similarity search and include the similarity scores\n",
    "retrieval_results = collection.query(\n",
    "    query_embeddings=[query_embedding],  \n",
    "    n_results=10,  # Retrieve top 5 results\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]  # Add similarity scores\n",
    ")\n",
    "\n",
    "# Display retrieved results with similarity scores\n",
    "for i, result in enumerate(retrieval_results[\"documents\"][0]):\n",
    "    similarity_score = 1 - retrieval_results[\"distances\"][0][i]  # Convert distance to similarity\n",
    "    print(f\"\\nResult {i+1}: (Similarity Score: {similarity_score:.4f})\")\n",
    "    print(f\"Title: {retrieval_results['metadatas'][0][i]['title']}\")\n",
    "    print(f\"Coordinates: {retrieval_results['metadatas'][0][i]['coordinates']}\")\n",
    "    print(f\"Categories: {retrieval_results['metadatas'][0][i]['categories']}\")\n",
    "    print(f\"Content: {result[:500]}...\")  # Show first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of chunks stored in ChromaDB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcollection\u001b[49m\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collection' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks stored in ChromaDB: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcollection\u001b[49m\u001b[38;5;241m.\u001b[39mget())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collection' is not defined"
     ]
    }
   ],
   "source": [
    "print(collection.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    OpenAI(temperature=0),\n",
    "    vectorstore.as_retriever(), # see below for vectorstore definition\n",
    "    memory=memory,\n",
    "    condense_question_prompt=condense_prompt,\n",
    "    combine_docs_chain_kwargs=dict(prompt=combine_docs_custom_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "condense_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Given the conversation history and the latest user query, rephrase the question to be standalone, keeping it concise but maintaining all necessary details.\n",
    "\n",
    "### Conversation History:\n",
    "{chat_history}\n",
    "\n",
    "### Latest User Query:\n",
    "{question}\n",
    "\n",
    "### Standalone Question for Retrieval:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_docs_custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are an AI travel planner helping users design an itinerary. Use the retrieved information about landmarks and the user's past preferences to generate a relevant and coherent travel recommendation.\n",
    "\n",
    "### Conversation History:\n",
    "{chat_history}\n",
    "\n",
    "### User's Latest Question:\n",
    "{question}\n",
    "\n",
    "### Retrieved Landmark Information:\n",
    "{context}\n",
    "\n",
    "### Instructions:\n",
    "- Provide a well-structured travel recommendation based on the retrieved landmarks.\n",
    "- Ensure continuity with previous discussions.\n",
    "- Prioritize landmarks that match the user’s preferences.\n",
    "- If multiple options exist, suggest the best ones with reasoning.\n",
    "- Avoid repeating information already given in the conversation.\n",
    "- In the end ask the user which of these locations you will like to visit.\n",
    "\n",
    "### Final Answer:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "OPENWEATHER_API_KEY = \"593b25670563b5c443274eb3677c971e\"\n",
    "\n",
    "def get_weather(city: str):\n",
    "    \"\"\"Fetches the current weather for a given city using OpenWeather API.\"\"\"\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"appid\": OPENWEATHER_API_KEY,\n",
    "        \"units\": \"metric\",  # Get temperature in Celsius\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            weather_description = data[\"weather\"][0][\"description\"]\n",
    "            temperature = data[\"main\"][\"temp\"]\n",
    "            humidity = data[\"main\"][\"humidity\"]\n",
    "            return f\"The weather in {city} is {weather_description} with a temperature of {temperature}°C and {humidity}% humidity.\"\n",
    "        else:\n",
    "            return f\"Error fetching weather for {city}: {data.get('message', 'Unknown error')}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"API request failed: {str(e)}\"\n",
    "\n",
    "# Define the tool for the agent\n",
    "weather_tool = Tool(\n",
    "    name=\"Weather Info\",\n",
    "    func=get_weather,\n",
    "    description=\"Provides real-time weather details for a given city.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "#Initialize LLM with ReAct Chain\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0, model=\"gpt-4-turbo\")\n",
    "\n",
    "agent_weather = initialize_agent(\n",
    "    tools=[weather_tool],  # Attach weather tool\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Enables reasoning & tool use\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "#tools = [weather_tool]\n",
    "\n",
    "\n",
    "#llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Define memory to store conversation history\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "\n",
    "# Function to print chat history\n",
    "def print_chat_history():\n",
    "    print(\"\\nChat History:\")\n",
    "    for idx, msg in enumerate(memory.chat_memory.messages):\n",
    "        role = \"User\" if msg.type == \"human\" else \"AI\"\n",
    "        print(f\"{role}: {msg.content}\")\n",
    "\n",
    "# Connect to your existing ChromaDB collection\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"landmarks_rag\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "#Define the retriever and chain\n",
    "retriever = vectorstore.as_retriever(k=3)  # This fetches relevant landmarks\n",
    "\n",
    "# Set up the conversational retrieval chain\n",
    "retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Using OpenAI's ChatGPT as the LLM\n",
    "    retriever=retriever,  # Connect to your ChromaDB retriever\n",
    "    memory=memory,\n",
    "    condense_question_prompt=condense_prompt,  # Ensures refined queries for retrieval\n",
    "    combine_docs_chain_kwargs=dict(prompt=combine_docs_custom_prompt)  # Customizes how retrieved docs are used\n",
    ")\n",
    "\n",
    "# Wrap QA Chain as a Tool \n",
    "qa_tool = Tool( name=\"Puerto Rico Travel Guide\", func=retrieval_chain.run, description=\"Retrieve the best places to visit in Puerto Rico based on user queries.\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "\n",
    "# =======================# 2. Extracting Locations from QA Response# ======================= \n",
    "location_extraction_prompt = PromptTemplate( input_variables=[\"response\"], \n",
    "                                            template=\"\"\" Extract only the location names from the following text: \"{response}\" Provide the locations as a comma-separated list. \"\"\" \n",
    "                                            ) \n",
    "                                            \n",
    "location_extraction_chain = LLMChain( llm=ChatOpenAI(model_name=\"gpt-4\"),\n",
    "                                     prompt=location_extraction_prompt )\n",
    "\n",
    "def extract_locations_from_response(response):\n",
    "    \"\"\"Extracts locations using the LLM chain.\"\"\"\n",
    "    location_list = location_extraction_chain.run(response)\n",
    "    return [loc.strip() for loc in location_list.split(\",\") if loc.strip()] \n",
    "\n",
    "# =======================# 3. Asking the User for Their Selected Places# =======================\n",
    "def ask_user_for_places(query): \n",
    "    \"\"\"Asks the user to select places they are interested in visiting.\"\"\"\n",
    "    # Retrieve recommended locations from QA #\n",
    "    recommended_places = qa_tool.run(query) \n",
    "    extracted_locations = extract_locations_from_response(recommended_places)\n",
    "    if extracted_locations:\n",
    "        weather_details = get_weather_for_selected_places(extracted_locations)\n",
    "        res = f\"Here are some great places to visit: {', '.join(extracted_locations)}. {weather_details}. Which ones do you want to visit?\"\n",
    "        return res\n",
    "    else:\n",
    "        return \"I couldn't find relevant locations. Please try another query.\"\n",
    "    # Wrap as a Tool\n",
    "\n",
    "ask_places_tool = Tool( name=\"Ask User for Selected Places\", func=ask_user_for_places, description=\"Ask the user which places they want to visit from the recommended list.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================# 4. Getting Weather for Selected Locations# =======================# \n",
    "def get_weather(location):\n",
    "    \"\"\"Fetch real-time weather for a given location.\"\"\"\n",
    "    api_key = \"593b25670563b5c443274eb3677c971e\"  # Replace with your API key\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    params = {\"q\": location, \"appid\": api_key, \"units\": \"metric\"}\n",
    "    response = requests.get(base_url, params=params) \n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        weather = data[\"weather\"][0][\"description\"]\n",
    "        temp = data[\"main\"][\"temp\"]\n",
    "        return f\"The weather in {location} is {weather} with a temperature of {temp}°C.\"\n",
    "    \n",
    "    else: return f\"Could not fetch weather data for {location}.\"\n",
    "    \n",
    "def get_weather_for_selected_places(selected_places):\n",
    "    \"\"\"Fetches weather for the user-selected locations.\"\"\"\n",
    "    locations = [loc.strip() for loc in selected_places.split(\",\") if loc.strip()]\n",
    "    if not locations:\n",
    "        return \"Please provide at least one valid location.\"\n",
    "    weather_reports = [get_weather(location) for location in locations]\n",
    "    return \"\\n\".join(weather_reports) \n",
    "\n",
    "# Wrap Weather Fetching as a Tool\n",
    "weather_tool = Tool( name=\"Get Weather for Selected Places\", func=get_weather_for_selected_places, description=\"Retrieve weather for the places selected by the user.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4\") \n",
    "agent = initialize_agent( tools=[qa_tool, ask_places_tool, weather_tool],\n",
    "                          # Adding all tools \n",
    "                          llm=llm,\n",
    "                          agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, # Keeps conversation context\n",
    "                          verbose=True,\n",
    "                          memory=memory \n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get recommended places\n",
    "response1 = agent.run(\"What are the best places to visit in Puerto Rico?\")\n",
    "\n",
    "print(response1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected: \"Here are some great places: San Juan, El Yunque, Culebra... Which ones do you want to visit?\"# Simulate User Input \n",
    "user_selected_places = \"San Juan and Ponce\"\n",
    "\n",
    "# Step 2: Fetch weather for selected places \n",
    "response2 = agent.run(f\"I want to visit {user_selected_places}\")\n",
    "\n",
    "print(response2) # Expected: Weather details for \"San Juan\" and \"El Yunque\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Fetch weather for selected places \n",
    "response2 = agent.run(\"what is the wether in those places?\")\n",
    "\n",
    "print(response2) # Expected: Weather details for \"San Juan\" and \"El Yunque\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m user_selected_places \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSan Juan, El Yunque\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Step 2: Fetch weather for selected places \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m response2 \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI want to visit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_selected_places\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(response2) \u001b[38;5;66;03m# Expected: Weather details for \"San Juan\" and \"El Yunque\".#User Query Example: Travel Plan\u001b[39;00m\n\u001b[0;32m      8\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the best places to visit in Puerto Rico?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "# Expected: \"Here are some great places: San Juan, El Yunque, Culebra... Which ones do you want to visit?\"# Simulate User Input \n",
    "user_selected_places = \"San Juan, El Yunque\"\n",
    "\n",
    "# Step 2: Fetch weather for selected places \n",
    "response2 = agent.run(f\"I want to visit {user_selected_places}\")\n",
    "\n",
    "print(response2) # Expected: Weather details for \"San Juan\" and \"El Yunque\".#User Query Example: Travel Plan\n",
    "question = \"What are the best places to visit in Puerto Rico?\"\n",
    "\n",
    "# Step 1: Retrieve locations from landmarks database (RAG)\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "# Extract most relevant location\n",
    "if retrieved_docs:\n",
    "    suggested_location = retrieved_docs[0].metadata.get(\"title\", \"San Juan\")  # Default to San Juan if no match\n",
    "else:\n",
    "    suggested_location = \"San Juan\"\n",
    "\n",
    "print(f\"Recommended Location: {suggested_location}\")\n",
    "\n",
    "# Step 2: Get Weather for Recommended Location\n",
    "weather_info = get_weather(suggested_location)\n",
    "print(f\"Weather Info: {weather_info}\")\n",
    "\n",
    "# Step 3: Generate Final Answer (LLM integrates both)\n",
    "final_query = f\"{question} Also, include the current temperature in {suggested_location}: {weather_info}.\"\n",
    "\n",
    "response = agent_weather.run(final_query)\n",
    "\n",
    "print(\"\\nAI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Query Example\n",
    "\n",
    "query_01 = \"What are the top landmarks I should visit in Puerto Rico based on my interests of beaches and nightlife?\"\n",
    "question = query_01\n",
    "\n",
    "# Retrieve documents separately so you can see them\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "#Print chat history BEFORE the query\n",
    "print_chat_history()\n",
    "\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for idx, doc in enumerate(retrieved_docs):\n",
    "    print(f\"{idx+1}. {doc.metadata.get('title', 'No Title')} - {doc.page_content[:200]}...\")\n",
    "\n",
    "# Run the retrieval chain with the retrieved documents\n",
    "response = retrieval_chain.run(question)\n",
    "print(\"\\n💡 AI Response:\")\n",
    "print(response)\n",
    "\n",
    "#Print chat history AFTER the query\n",
    "print_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Query Example\n",
    "\n",
    "query_02 = \"I will like to visit Condado Beach and El Yunque.\"\n",
    "question = query_02\n",
    "\n",
    "# Retrieve documents separately so you can see them\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "#Print chat history BEFORE the query\n",
    "print_chat_history()\n",
    "\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for idx, doc in enumerate(retrieved_docs):\n",
    "    print(f\"{idx+1}. {doc.metadata.get('title', 'No Title')} - {doc.page_content[:200]}...\")\n",
    "\n",
    "# Run the retrieval chain with the retrieved documents\n",
    "response = retrieval_chain.run(question)\n",
    "print(\"\\n💡 AI Response:\")\n",
    "print(response)\n",
    "\n",
    "#Print chat history AFTER the query\n",
    "print_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Query Example\n",
    "\n",
    "query_02 = \"I also what to see museums.\"\n",
    "question = query_02\n",
    "\n",
    "# Retrieve documents separately so you can see them\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "#Print chat history BEFORE the query\n",
    "print_chat_history()\n",
    "\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for idx, doc in enumerate(retrieved_docs):\n",
    "    print(f\"{idx+1}. {doc.metadata.get('title', 'No Title')} - {doc.page_content[:200]}...\")\n",
    "\n",
    "# Run the retrieval chain with the retrieved documents\n",
    "response = retrieval_chain.run(question)\n",
    "print(\"\\n💡 AI Response:\")\n",
    "print(response)\n",
    "\n",
    "#Print chat history AFTER the query\n",
    "print_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Query Example\n",
    "\n",
    "query_03 = \"Create an itinerary for me.\"\n",
    "question = query_03\n",
    "\n",
    "# Retrieve documents separately so you can see them\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "#Print chat history BEFORE the query\n",
    "print_chat_history()\n",
    "\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for idx, doc in enumerate(retrieved_docs):\n",
    "    print(f\"{idx+1}. {doc.metadata.get('title', 'No Title')} - {doc.page_content[:200]}...\")\n",
    "\n",
    "# Run the retrieval chain with the retrieved documents\n",
    "response = retrieval_chain.run(question)\n",
    "print(\"\\n💡 AI Response:\")\n",
    "print(response)\n",
    "\n",
    "#Print chat history AFTER the query\n",
    "print_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block 1: Import Required Libraries\n",
    "\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_community.utilities.openweathermap import OpenWeatherMapAPIWrapper\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API keys\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "OPENWEATHER_API_KEY = os.getenv('OPENWEATHER_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Fetch Weather Data\n",
    "\n",
    "def get_weather(city: str):\n",
    "    \"\"\"Fetches real-time weather data for a given city using OpenWeather API.\"\"\"\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"appid\": OPENWEATHER_API_KEY,\n",
    "        \"units\": \"metric\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            weather_description = data[\"weather\"][0][\"description\"]\n",
    "            temperature = data[\"main\"][\"temp\"]\n",
    "            humidity = data[\"main\"][\"humidity\"]\n",
    "            return f\"The current temperature in {city} is {temperature}°C with {weather_description} and {humidity}% humidity.\"\n",
    "        else:\n",
    "            return f\"Error fetching weather for {city}: {data.get('message', 'Unknown error')}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"API request failed: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_tool = tool(get_weather)\n",
    "\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0, model=\"gpt-4-turbo\")\n",
    "\n",
    "agent_weather = initialize_agent(\n",
    "    tools=[weather_tool],  # Attach weather tool\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Enables reasoning & tool use\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "\n",
    "def print_chat_history():\n",
    "    \"\"\"Displays stored chat history for debugging.\"\"\"\n",
    "    print(\"\\nChat History:\")\n",
    "    for idx, msg in enumerate(memory.chat_memory.messages):\n",
    "        role = \"User\" if msg.type == \"human\" else \"AI\"\n",
    "        print(f\"{role}: {msg.content}\")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"landmarks_rag\",\n",
    "    embedding_function=OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY)\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=3)  # Retrieves top 3 relevant landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travel Recommendation Chain\n",
    "\n",
    "travel_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a travel planner to visit Puerto Rico . Based on the conversation history and user question, recommend the best landmarks to visit in a structured day-by-day itinerary.\n",
    "\n",
    "### Chat History:\n",
    "{chat_history}\n",
    "\n",
    "### User Question:\n",
    "{question}\n",
    "\n",
    "### Travel Recommendation:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "travel_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type=\"stuff\",  # Ensures all retrieved docs are passed to the LLM\n",
    "    condense_question_prompt=travel_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location Extraction Chain\n",
    "\n",
    "location_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"travel_response\"],\n",
    "    template=\"\"\"\n",
    "Extract the names of locations mentioned in the travel response.\n",
    "\n",
    "### Travel Recommendation:\n",
    "{travel_response}\n",
    "\n",
    "### Extracted Locations (Comma-Separated):\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "location_extraction_chain = LLMChain(llm=llm, prompt=location_extraction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itinerary Generator\n",
    "\n",
    "def generate_itinerary(selected_places, num_days):\n",
    "    \"\"\"Generates a structured itinerary based on user preferences and trip duration.\"\"\"\n",
    "    itinerary = f\"\\n🏝️ **Your {num_days}-Day Itinerary** 🏝️\\n\"\n",
    "    places_per_day = max(1, len(selected_places) // num_days)\n",
    "    random.shuffle(selected_places)\n",
    "\n",
    "    for day in range(1, num_days + 1):\n",
    "        start_idx = (day - 1) * places_per_day\n",
    "        end_idx = start_idx + places_per_day\n",
    "        day_places = selected_places[start_idx:end_idx]\n",
    "\n",
    "        if not day_places:\n",
    "            break\n",
    "\n",
    "        itinerary += f\"\\n📅 **Day {day}:**\\n\"\n",
    "        for place in day_places:\n",
    "            itinerary += f\"- Visit **{place}**\\n\"\n",
    "\n",
    "    return itinerary.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational Process & Final Recommendation\n",
    "\n",
    "def travel2pr_chatbot():\n",
    "    \"\"\"Handles user interaction until the itinerary is confirmed.\"\"\"\n",
    "    confirmed = False\n",
    "    while not confirmed:\n",
    "        question = input(\"\\nUser: \")  # User enters their query\n",
    "        travel_response = travel_chain.invoke({\"question\": question})\n",
    "\n",
    "        print(\"\\n💡 AI Travel Recommendation:\\n\", travel_response[\"answer\"])\n",
    "\n",
    "        # Extract locations from response (FIXED)\n",
    "        locations = extract_locations_from_response(travel_response[\"answer\"])\n",
    "        \n",
    "        if not locations:\n",
    "            print(\"\\n⚠️ No valid locations found. Try asking about specific places.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Infer trip duration from conversation or ask\n",
    "        num_days = None\n",
    "        for msg in reversed(memory.chat_memory.messages):\n",
    "            if \"days\" in msg.content.lower():\n",
    "                try:\n",
    "                    words = msg.content.split()\n",
    "                    for word in words:\n",
    "                        if word.isdigit():\n",
    "                            num_days = int(word)\n",
    "                            break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "        if num_days is None:\n",
    "            num_days = int(input(\"\\nHow many days is your trip? \"))\n",
    "            memory.chat_memory.add_user_message(f\"My trip is {num_days} days.\")\n",
    "\n",
    "        # Fetch weather info for each location\n",
    "        weather_responses = {location: get_weather(location) for location in locations}\n",
    "\n",
    "        # Generate final structured itinerary\n",
    "        itinerary = generate_itinerary(locations, num_days)\n",
    "\n",
    "        # Display final AI response\n",
    "        print(\"\\n📅 **Final Itinerary:**\\n\", itinerary)\n",
    "        print(\"\\n🌤 **Weather Report:**\\n\", \"\\n\".join([f\"{loc}: {weather}\" for loc, weather in weather_responses.items()]))\n",
    "\n",
    "        # Confirm itinerary\n",
    "        user_feedback = input(\"\\nDoes this itinerary look good? (yes/no): \").strip().lower()\n",
    "        if user_feedback == \"yes\":\n",
    "            confirmed = True\n",
    "            print(\"\\n✅ **Itinerary Finalized! Have a great trip!**\")\n",
    "        else:\n",
    "            print(\"\\n🔄 Adjusting itinerary... Let's refine your plan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 AI Travel Recommendation:\n",
      " It looks like you've already provided a detailed and well-thought-out itinerary for a trip to Puerto Rico! If you have any specific questions or need further information about any of the places mentioned or activities suggested, feel free to ask. Enjoy your trip planning!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_locations_from_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the chatbot\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtravel2pr_chatbot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m, in \u001b[0;36mtravel2pr_chatbot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m💡 AI Travel Recommendation:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, travel_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Extract locations from response (FIXED)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m locations \u001b[38;5;241m=\u001b[39m \u001b[43mextract_locations_from_response\u001b[49m(travel_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m locations:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m⚠️ No valid locations found. Try asking about specific places.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_locations_from_response' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the chatbot\n",
    "travel2pr_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "travel_planning_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-RAG Verification Code (Full Implementation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the path to the vectorstore (RAG system)\n",
    "VECTORSTORE_PATH = r\"C:\\Users\\larry\\chromadb_store\\landmarks_db\"\n",
    "\n",
    "# Initialize embeddings and vectorstore\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectorstore = Chroma(persist_directory=VECTORSTORE_PATH, embedding_function=embedding_model)\n",
    "\n",
    "# Define test queries for RAG verification\n",
    "test_queries = [\n",
    "    \"Where is El Yunque National Forest located?\",\n",
    "    \"Tell me about Castillo San Felipe del Morro.\",\n",
    "    \"What are the most famous beaches in Puerto Rico?\",\n",
    "    \"Which landmarks should I visit in Old San Juan?\",\n",
    "    \"What are the best hiking trails in Puerto Rico?\"\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store retrieval results\n",
    "retrieval_results = []\n",
    "\n",
    "# Function to check retrieval quality\n",
    "def evaluate_retrieval(query):\n",
    "    \"\"\"Retrieves documents from the RAG system using collection.query and logs similarity scores.\"\"\"\n",
    "    \n",
    "    # Ensure the query is embedded before searching\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "    # Perform a similarity search using collection.query\n",
    "    retrieval_data = vectorstore._collection.query(\n",
    "        query_embeddings=[query_embedding],  \n",
    "        n_results=5,  # Retrieve top 5 most relevant results\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]  # Include similarity scores\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = []\n",
    "\n",
    "    for i, result in enumerate(retrieval_data[\"documents\"][0]):\n",
    "        similarity_score = 1 - retrieval_data[\"distances\"][0][i]  # Convert distance to similarity\n",
    "        chunk_data = {\n",
    "            \"query\": query,\n",
    "            \"retrieved_text\": result[:300] + \"...\",  # Show only first 300 characters\n",
    "            \"source_title\": retrieval_data[\"metadatas\"][0][i][\"title\"],\n",
    "            \"coordinates\": retrieval_data[\"metadatas\"][0][i][\"coordinates\"],\n",
    "            \"categories\": retrieval_data[\"metadatas\"][0][i][\"categories\"],\n",
    "            \"similarity_score\": round(similarity_score, 4)  # Format similarity score\n",
    "        }\n",
    "        retrieved_chunks.append(chunk_data)\n",
    "\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Run retrieval tests on all queries\n",
    "for query in test_queries:\n",
    "    retrieval_results.extend(evaluate_retrieval(query))\n",
    "\n",
    "# Convert results to DataFrame for better readability\n",
    "retrieval_df = pd.DataFrame(retrieval_results)\n",
    "\n",
    "# Display verification results\n",
    "print(\"\\n=== Pre-RAG Verification Results ===\")\n",
    "print(retrieval_df.to_string(index=False))\n",
    "\n",
    "# Save results to a CSV file for further analysis\n",
    "retrieval_df.to_csv(r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\RAG Tests\\lm_test03\\rag_verification_results.csv\", index=False)\n",
    "\n",
    "print(\"\\nRAG verification results saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of chunks stored in ChromaDB: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorstore.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "VECTORSTORE_PATH = r\"C:\\Users\\larry\\chromadb_store\\landmarks_db\"\n",
    "\n",
    "# Reload Chroma with persistence\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=VECTORSTORE_PATH,\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# Check the number of stored documents\n",
    "print(f\"Total documents stored: {vectorstore._collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ChromaDB collection actually contains the expected embeddings\n",
    "print(f\"Number of chunks stored in ChromaDB: {collection.count()}\")\n",
    "\n",
    "# Retrieve an arbitrary document to check data integrity\n",
    "test_retrieval = collection.query(\n",
    "    query_embeddings=[embedding_model.embed_query(\"Test query\")],  \n",
    "    n_results=1,  \n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"\\nSample Retrieved Document:\\n\", test_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar un embedding de prueba\n",
    "test_embedding = embedding_model.embed_query(\"Test query\")\n",
    "\n",
    "print(f\"Sample embedding vector (length: {len(test_embedding)}): {test_embedding[:5]} ...\")  # Muestra los primeros 5 valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera embeddings para los documentos (asegúrate de usar una lista de textos)\n",
    "embeddings_list = embedding_model.embed_documents([doc[\"content\"] for doc in chunked_documents])\n",
    "\n",
    "# Verifica la cantidad de embeddings generados\n",
    "print(f\"Total embeddings generated: {len(embeddings_list)}\")\n",
    "print(f\"First embedding vector sample (length: {len(embeddings_list[0])}): {embeddings_list[0][:5]} ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si los embeddings están almacenados en ChromaDB\n",
    "sample_query = embedding_model.embed_query(\"Test query\")\n",
    "test_retrieval = collection.query(\n",
    "    query_embeddings=[sample_query],\n",
    "    n_results=1,\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(\"\\nSample Retrieved Document:\\n\", test_retrieval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel2pr_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
